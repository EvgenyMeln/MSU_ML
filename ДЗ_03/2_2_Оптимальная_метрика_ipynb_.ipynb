{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Оптимальная метрика\n",
        "\n",
        "В этом задании Вам предлагается подобрать оптимальную метрику и оптимальное значение гиперпараметра K из диапазона [1,50] для решения задачи классификации на примере датасета Ирисов Фишера. Этот датасет можно загрузить из модуля sklearn.datasets.\n",
        "\n",
        "Качества оценивается при помощи метрики accuracy при помощи методики кросс-валидации. Об этой методике можно подробнее прочитать в [документации sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html).\n",
        "\n",
        "Мы предлагаем Вам заполнить недостающие команды в следующем скелете кода и разобраться, какую метрику оптимально применять для решения данной задачи. В ответе на задание необходимо указать эту метрику.\n",
        "\n",
        "Попробуйте 3 варианта: манхэттенское расстояние, евклидово расстояние и косинусное расстояние. Полный список возможных метрик можно посмотреть по [ссылке](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.distance_metrics.html#sklearn.metrics.pairwise.distance_metrics). Меняйте этот параметр, изменяя значение аргумента `metric` при создании объекта класса `KNeighborsClassifier`. Найдите пару \"метрика\"-\"K\", для которой получается наилучшее качество и в качестве ответа укажите **найденную метрику**\n",
        "\n",
        "**Замечание**: параметр *n_splits* - это количество разбиений `cv` в кросс-валидации. В качестве итоговой метрики берётся усреднение полученных значений метрик по всем разбиениям."
      ],
      "metadata": {
        "id": "Vul_oMFvIqYL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "random_seed = 4238\n",
        "\n",
        "np.random.seed(random_seed)\n",
        "n_splits = 3\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "X, y = load_iris(return_X_y=True)\n",
        "scores = []\n",
        "\n",
        "\"\"\"\n",
        "  Здесь Вам предлагается написать тело цикла для подбора оптимального K\n",
        "  Результаты оценки алгоритма при каждом отдельно взятом K рекомендуем записывать в список cv_scores\n",
        "\"\"\"\n",
        "for k in range(1, 51):\n",
        "    # TODO: Write Your Code Here\n",
        "    clf = KNeighborsClassifier(n_neighbors=k,metric='euclidean')\n",
        "    score = cross_val_score(clf, X, y, cv=n_splits)\n",
        "    print(score)\n",
        "    scores.append(np.mean(score))\n",
        "    pass\n",
        "print(scores)\n",
        "k_best = np.argmax(scores) + 1\n",
        "print(k_best)"
      ],
      "metadata": {
        "id": "G9zTXC4TIsoG",
        "outputId": "0674c7a1-c83a-4b06-dec8-897c9ffe2bcc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.98 0.94 0.96]\n",
            "[0.96 0.94 0.94]\n",
            "[0.98 0.96 0.98]\n",
            "[0.98 1.   0.98]\n",
            "[0.98 0.98 0.98]\n",
            "[0.98 0.98 0.96]\n",
            "[0.98 0.98 0.96]\n",
            "[0.98 0.98 0.94]\n",
            "[0.96 1.   0.96]\n",
            "[0.96 1.   0.94]\n",
            "[0.96 1.   0.94]\n",
            "[0.96 1.   0.94]\n",
            "[0.96 0.96 0.96]\n",
            "[0.96 0.96 0.94]\n",
            "[0.96 0.96 0.94]\n",
            "[0.96 0.98 0.94]\n",
            "[0.96 0.96 0.96]\n",
            "[0.96 0.96 0.94]\n",
            "[0.96 0.96 0.94]\n",
            "[0.96 0.92 0.96]\n",
            "[0.96 0.94 0.96]\n",
            "[0.96 0.92 0.96]\n",
            "[0.94 0.94 0.94]\n",
            "[0.94 0.92 0.94]\n",
            "[0.94 0.94 0.94]\n",
            "[0.96 0.94 0.96]\n",
            "[0.96 0.94 0.96]\n",
            "[0.96 0.94 0.96]\n",
            "[0.96 0.94 0.96]\n",
            "[0.96 0.94 0.96]\n",
            "[0.96 0.94 0.96]\n",
            "[0.96 0.92 0.96]\n",
            "[0.94 0.92 0.96]\n",
            "[0.94 0.88 0.94]\n",
            "[0.94 0.9  0.94]\n",
            "[0.92 0.92 0.92]\n",
            "[0.92 0.92 0.92]\n",
            "[0.9  0.92 0.92]\n",
            "[0.9  0.92 0.92]\n",
            "[0.9  0.92 0.9 ]\n",
            "[0.9  0.92 0.92]\n",
            "[0.9 0.9 0.9]\n",
            "[0.9  0.9  0.92]\n",
            "[0.9  0.9  0.88]\n",
            "[0.92 0.9  0.88]\n",
            "[0.9  0.88 0.88]\n",
            "[0.9  0.9  0.88]\n",
            "[0.9  0.88 0.88]\n",
            "[0.9  0.88 0.88]\n",
            "[0.9  0.88 0.88]\n",
            "[0.96, 0.9466666666666667, 0.9733333333333333, 0.9866666666666667, 0.98, 0.9733333333333333, 0.9733333333333333, 0.9666666666666667, 0.9733333333333333, 0.9666666666666667, 0.9666666666666667, 0.9666666666666667, 0.96, 0.9533333333333333, 0.9533333333333333, 0.96, 0.96, 0.9533333333333333, 0.9533333333333333, 0.9466666666666667, 0.9533333333333333, 0.9466666666666667, 0.94, 0.9333333333333332, 0.94, 0.9533333333333333, 0.9533333333333333, 0.9533333333333333, 0.9533333333333333, 0.9533333333333333, 0.9533333333333333, 0.9466666666666667, 0.94, 0.9199999999999999, 0.9266666666666666, 0.92, 0.92, 0.9133333333333334, 0.9133333333333334, 0.9066666666666667, 0.9133333333333334, 0.9, 0.9066666666666667, 0.8933333333333334, 0.9, 0.8866666666666667, 0.8933333333333334, 0.8866666666666667, 0.8866666666666667, 0.8866666666666667]\n",
            "4\n"
          ]
        }
      ]
    }
  ]
}